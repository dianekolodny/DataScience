{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianekolodny/DataScience/blob/main/Riboswitches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm0GiJvLM4cN"
      },
      "source": [
        "Colab Pro, Premium GPU (A100), High RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtSz1S3pN5ml"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uexGuTWRNKyp"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH44_cnQOe7W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchVision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uBpu4igd9Kh"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "The first step is to prepare the RNA sequence and 3D structure data for training and evaluation. We will use the Rfam database to obtain RNA sequences belonging to known Riboswitch classes. We can store this data in a CSV file or other format that is suitable for loading into a PyTorch DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc-0biAEcV4c"
      },
      "outputs": [],
      "source": [
        "!pip install pandas torch requests bs4 transformers pytorch-ranger ranger #json re gzip io csv matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVOuG7kFGUQg"
      },
      "source": [
        "### Create a CSV for all RFAM IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLh23GAp_orx"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Define the data as a list of tuples\n",
        "riboswitch_data = [\n",
        "    ('Glutamine riboswitch', 'RF01739'),\n",
        "    ('Glutamine-II riboswitch (downstream peptide RNA)', 'RF01704'),\n",
        "    ('raiA RNA', 'RF03072'),\n",
        "    ('SAM riboswitch (S box leader)', 'RF00162'),\n",
        "    ('Cobalamin riboswitch', 'RF00174'),\n",
        "    ('SAM-I/IV variant riboswitch', 'RF01725'),\n",
        "    ('Fluoride riboswitch (crcB)', 'RF01734'),\n",
        "    ('nhaA-I RNA', 'RF03057'),\n",
        "    ('ZMP/ZTP riboswitch', 'RF01750'),\n",
        "    ('Moco (molybdenum cofactor) riboswitch', 'RF01055'),\n",
        "    ('M-box riboswitch (ykoK leader)', 'RF00380'),\n",
        "    ('FMN riboswitch (RFN element)', 'RF00050'),\n",
        "    ('Na+ riboswitch (DUF1646 RNA)', 'RF03071'),\n",
        "    ('AdoCbl variant RNA', 'RF01689'),\n",
        "    ('Purine riboswitch', 'RF00167'),\n",
        "    ('TPP riboswitch (THI element)', 'RF00059'),\n",
        "    ('ydaO/yuaA leader', 'RF00379'),\n",
        "    ('THF riboswitch', 'RF01831'),\n",
        "    ('Guanidine-I riboswitch', 'RF00442'),\n",
        "    ('NiCo riboswitch', 'RF02683'),\n",
        "    ('sul1 RNA', 'RF03058'),\n",
        "    ('SAM/SAH riboswitch', 'RF01727'),\n",
        "    ('Cyclic di-GMP-II riboswitch', 'RF01786'),\n",
        "    ('S-adenosyl-L-homocysteine riboswitch', 'RF01057'),\n",
        "    ('Lysine riboswitch', 'RF00168'),\n",
        "    ('Glycine riboswitch', 'RF00504'),\n",
        "    ('S-adenosyl methionine (SAM) riboswitch', 'RF00634'),\n",
        "    ('SAM riboswitch (alpha-proteobacteria)', 'RF00521'),\n",
        "    ('PreQ1 riboswitch', 'RF00522'),\n",
        "    ('PreQ1-III riboswitch', 'RF02680'),\n",
        "    ('yybP-ykoY manganese riboswitch', 'RF00080'),\n",
        "    ('SMK box translational riboswitch (SAM-III)', 'RF01767'),\n",
        "    ('glmS glucosamine-6-phosphate activated ribozyme', 'RF00234'),\n",
        "    ('preQ1-II (pre queuosine) riboswitch', 'RF01054'),\n",
        "    ('SAM-VI riboswitch', 'RF02885'),\n",
        "    ('SAM-V riboswitch', 'RF01826'),\n",
        "    ('AdoCbl riboswitch', 'RF01482'),\n",
        "    ('Magnesium Sensor', 'RF01056'),\n",
        "    ('putative aminoglycoside riboswitch / attI site', 'RF02912'),\n",
        "    ('M. florum riboswitch', 'RF01510')\n",
        "]\n",
        "\n",
        "# Define the headers for the CSV file\n",
        "csv_headers = ['family_name', 'rfam_id', 'rfam_id_number_only']\n",
        "\n",
        "# Define a function to extract the non-character integers from the rfam_id\n",
        "def extract_integers(string):\n",
        "    return ''.join(c for c in string if c.isdigit())\n",
        "\n",
        "# Create a list of tuples with the family_name, rfam_id, and rfam_id_number_only columns\n",
        "processed_data = [(family_name, rfam_id, extract_integers(rfam_id)) for family_name, rfam_id in riboswitch_data]\n",
        "\n",
        "# Write the data to a CSV file\n",
        "with open('riboswitch_data.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(csv_headers)\n",
        "    writer.writerows(processed_data)\n",
        "\n",
        "print('CSV file created successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1hUTbUmz9eV"
      },
      "source": [
        "We will take the starting file and download all possible sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZKS5DfOvt0b"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Load the riboswitch_data.csv file\n",
        "riboswitch_data = pd.read_csv('riboswitch_data.csv')\n",
        "\n",
        "# Initialize an empty dataframe for the riboswitch dataset\n",
        "riboswitch_dataset = pd.DataFrame(columns=['Accession ID', 'Start-End', 'Description', 'Sequence', 'rfam_id', 'rfam_id_number_only'])\n",
        "\n",
        "# Define a function to parse and process the downloaded sequences\n",
        "def process_sequence_data(sequence_data, rfam_id, rfam_id_number_only):\n",
        "    # Initialize a list to store processed sequences\n",
        "    sequences = []\n",
        "\n",
        "    # Initialize variables for storing data\n",
        "    accession_id = ''\n",
        "    start_end = ''\n",
        "    description = ''\n",
        "    sequence = ''\n",
        "\n",
        "    # Loop over lines in the sequence_data\n",
        "    for line in sequence_data.split('\\n'):\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check if line starts with '>'\n",
        "        if line.startswith('>'):\n",
        "            # If not the first sequence, append the previous data to the list\n",
        "            if accession_id:\n",
        "                sequences.append([accession_id, start_end, description, sequence, rfam_id, rfam_id_number_only])\n",
        "\n",
        "            # Extract accession ID, start-end, and description from line\n",
        "            accession_id = line[1:].split('/')[0]\n",
        "            start_end = line[1:].split('/')[1].split(' ')[0]\n",
        "            description = ' '.join(line[1:].split('/')[1].split(' ')[1:])\n",
        "\n",
        "            # Reset sequence variable\n",
        "            sequence = ''\n",
        "        else:\n",
        "            # Append sequence to variable\n",
        "            sequence += line\n",
        "\n",
        "    # Append the last sequence to the list\n",
        "    sequences.append([accession_id, start_end, description, sequence, rfam_id, rfam_id_number_only])\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Loop through all RFAM IDs in the riboswitch_data.csv file\n",
        "for index, row in riboswitch_data.iterrows():\n",
        "    rfam_id = row['rfam_id']\n",
        "    rfam_id_number_only = row['rfam_id_number_only']\n",
        "\n",
        "    # Download the sequences\n",
        "    url = f'https://rfam.org/family/{rfam_id}/alignment?acc={rfam_id}&format=fastau&download=0'\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        # Unzip the downloaded data\n",
        "        sequence_data = response.content.decode('utf-8')\n",
        "\n",
        "        # Process the sequence data and append it to the riboswitch_dataset dataframe\n",
        "        processed_data = process_sequence_data(sequence_data, rfam_id, rfam_id_number_only)\n",
        "        riboswitch_dataset = pd.concat([riboswitch_dataset, pd.DataFrame(processed_data, columns=riboswitch_dataset.columns)], ignore_index=True)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download sequences for {rfam_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Save the riboswitch dataset to a CSV file\n",
        "riboswitch_dataset.to_csv('riboswitch_dataset.csv', index=False)\n",
        "\n",
        "print('Riboswitch dataset created successfully!')\n",
        "# Print top 6 rows of riboswitch_dataset.csv\n",
        "print(riboswitch_dataset.head(6))\n",
        "\n",
        "print('Original Riboswitch data sample below!')\n",
        "# Print top 6 rows of riboswitch_data.csv\n",
        "print(riboswitch_data.head(6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmnxDmGdROR-"
      },
      "source": [
        "We will take the resulting file and validate the records in our CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDLuvANPIA6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the riboswitch dataset file\n",
        "riboswitch_dataset = pd.read_csv('riboswitch_dataset.csv')\n",
        "\n",
        "# Group the data by rfam_id and count the unique Accession IDs or Sequences per rfam_id\n",
        "#accession_counts = riboswitch_dataset.groupby('rfam_id')['Accession ID'].nunique()\n",
        "unique_sequence_counts = riboswitch_dataset.groupby('rfam_id')['Sequence'].nunique()\n",
        "all_sequence_counts = riboswitch_dataset.groupby('rfam_id')['Sequence']\n",
        "# Print the number of unique Accession IDs per rfam_id\n",
        "print(all_sequence_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWmeS4Q3_1Zd"
      },
      "outputs": [],
      "source": [
        "#next we prepare the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "\n",
        "class RiboswitchDataset(Dataset):\n",
        "    def __init__(self, df, max_seq_len):\n",
        "        self.df = df\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.label_to_int = {label: i for i, label in enumerate(sorted(self.df['rfam_id'].unique()))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rna_seq = self.df.loc[idx, 'Sequence']\n",
        "        one_hot_rna = torch.zeros(self.max_seq_len, 4)\n",
        "\n",
        "        for i, base in enumerate(rna_seq):\n",
        "            if i >= self.max_seq_len:\n",
        "                break\n",
        "            if base == 'A':\n",
        "                one_hot_rna[i, 0] = 1\n",
        "            elif base == 'C':\n",
        "                one_hot_rna[i, 1] = 1\n",
        "            elif base == 'G':\n",
        "                one_hot_rna[i, 2] = 1\n",
        "            elif base == 'U':\n",
        "                one_hot_rna[i, 3] = 1\n",
        "\n",
        "        label = self.label_to_int[self.df.loc[idx, 'rfam_id']]\n",
        "        return one_hot_rna, label\n",
        "\n",
        "# Load Riboswitch dataset from CSV file with padding\n",
        "max_seq_len = 200  # You can set this value according to your dataset\n",
        "riboswitch_df = pd.read_csv('riboswitch_dataset.csv')\n",
        "riboswitch_dataset = RiboswitchDataset(riboswitch_df, max_seq_len=max_seq_len)\n",
        "val_size = int(len(riboswitch_dataset) * 0.2)\n",
        "train_size = len(riboswitch_dataset) - val_size\n",
        "\n",
        "# Randomly split dataset into training and validation subsets\n",
        "train_dataset, val_dataset = random_split(riboswitch_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnWdnBIUeMjY"
      },
      "source": [
        "## Model Architecture (Transformer)\n",
        "\n",
        "We will implement the modified transformer model that incorporates a graph neural network layer, as described in the prior framework you provided. We will use PyTorch to define the architecture of the model, including the input encoding layers, graph neural network layer, multi-head self-attention layer, positional encoding, feedforward layer, and classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzBXZrrO2T86"
      },
      "outputs": [],
      "source": [
        "#Define your model architecture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "class RiboswitchClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, max_seq_len):\n",
        "        super(RiboswitchClassifier, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Embedding(4, 16)\n",
        "        self.rnn = nn.GRU(input_size=16, hidden_size=32, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(32 * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x.argmax(dim=-1))\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "num_classes = len(train_dataset.dataset.label_to_int)\n",
        "model = RiboswitchClassifier(num_classes, max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yOSy1FfpWB0Q"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Add data augmentations\n",
        "def random_reverse(sequence):\n",
        "    if torch.rand(1) < 0.5:\n",
        "        return sequence.flip(dims=[1])\n",
        "    return sequence\n",
        "\n",
        "def random_shuffle(sequence):\n",
        "    if torch.rand(1) < 0.5:\n",
        "        perm = torch.randperm(sequence.size(1))\n",
        "        return sequence[:, perm]\n",
        "    return sequence\n",
        "\n",
        "def random_complement(sequence):\n",
        "    if torch.rand(1) < 0.5:\n",
        "        complement_map = {0: 3, 1: 2, 2: 1, 3: 0}\n",
        "        return sequence.flip(dims=[2]).apply_(lambda x: complement_map[int(x)])\n",
        "    return sequence\n",
        "\n",
        "def apply_augmentations(sequence):\n",
        "    sequence = random_reverse(sequence)\n",
        "    sequence = random_shuffle(sequence)\n",
        "    sequence = random_complement(sequence)\n",
        "    return sequence\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-5  # for L2 regularization\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use RAdam optimizer\n",
        "from pytorch_ranger import Ranger\n",
        "optimizer = Ranger(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = apply_augmentations(inputs)  # Apply data augmentations\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add current batch loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    epoch_loss = running_loss / (i + 1)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for j, data in enumerate(val_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Calculate validation loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_loss = val_loss / (j + 1)  # Calculate average validation loss for the epoch\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3KbGQz2T8KW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        input_shape = input_ids.size()\n",
        "        if len(input_shape) > 2:\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "        bert_output = self.bert(input_ids)\n",
        "        pooled_output = bert_output[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits    \n",
        "\n",
        "#model = TransformerModel(num_classes=2)\n",
        "# input_ids = torch.ones((2, 5), dtype=torch.long)  # Example input tensor of shape (batch_size, sequence_length)\n",
        "# logits = model(input_ids)\n",
        "# print(logits.shape)  # Should print (2, 2) for binary classification with batch_size=2\n",
        "\n",
        "# Use the RiboswitchDataset and DataLoader code you provided earlier\n",
        "\n",
        "# Define training and validation loop\n",
        "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, label_names):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.squeeze()).sum().item()\n",
        "\n",
        "                # Store true labels and predicted labels\n",
        "                if epoch == num_epochs - 1:\n",
        "                    y_true.extend(labels.squeeze().tolist())\n",
        "                    y_pred.extend(predicted.tolist())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "    \n",
        "    return y_true, y_pred, label_names\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "num_classes = len(riboswitch_df['rfam_id'].unique())\n",
        "model = TransformerModel(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Add a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 100\n",
        "label_names = riboswitch_df[\"rfam_id\"].unique()\n",
        "y_true, y_pred, label_names = train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, label_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUph_h90UcgU"
      },
      "source": [
        "### pause, everthing below in this section is wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7m_XAtU24CK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RiboswitchTransformer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RiboswitchTransformer, self).__init__()\n",
        "        \n",
        "        # Input Encoding Layers\n",
        "        self.embedding = nn.Embedding(num_embeddings=4, embedding_dim=128)\n",
        "        \n",
        "        # Graph Neural Network Layer\n",
        "        # Graph Construction\n",
        "        # Node and Edge Features\n",
        "        \n",
        "        # Graph Neural Network Architecture\n",
        "        self.gnn_layer = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Output Features\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        self.self_attn_layer = nn.MultiheadAttention(embed_dim=32, num_heads=4)\n",
        "        \n",
        "        # Positional Encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(100, 1, 32))\n",
        "        \n",
        "        # Feedforward Layer\n",
        "        self.feedforward_layer = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Classification Layer\n",
        "        self.classification_layer = nn.Linear(32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Input Encoding Layers\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # Graph Neural Network Layer\n",
        "        # Graph Construction\n",
        "        # Node and Edge Features\n",
        "        \n",
        "        # Graph Neural Network Architecture\n",
        "        x = self.gnn_layer(x)\n",
        "        \n",
        "        # Output Features\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        x = x.transpose(0, 1)\n",
        "        x, _ = self.self_attn_layer(x, x, x)\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        # Positional Encoding\n",
        "        x = x + self.pos_encoding[:x.size(0)]\n",
        "        \n",
        "        # Feedforward Layer\n",
        "        x = self.feedforward_layer(x)\n",
        "        \n",
        "        # Classification Layer\n",
        "        x = self.classification_layer(x[:, -1, :])\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHef8fb0DXkS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class RiboswitchDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load RNA sequence and label from dataframe\n",
        "        rna_seq = self.df.loc[idx, 'Sequence']\n",
        "        rfam_id = self.df.loc[idx, 'rfam_id']\n",
        "        \n",
        "        # Convert RNA sequence to tensor\n",
        "        rna_seq_tensor = torch.tensor([int(c == x) for x in \"ATCG\" for c in rna_seq])\n",
        "        \n",
        "        # Convert class label to tensor\n",
        "        class_label_tensor = torch.tensor(int(rfam_id))\n",
        "        \n",
        "        return rna_seq_tensor, class_label_tensor\n",
        "\n",
        "# Load Riboswitch dataset from CSV file\n",
        "riboswitch_dataset = RiboswitchDataset('riboswitch_dataset.csv')\n",
        "\n",
        "# Split dataset into training and validation subsets\n",
        "val_size = int(len(riboswitch_dataset) * 0.2)\n",
        "train_size = len(riboswitch_dataset) - val_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(riboswitch_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "class RiboswitchTransformer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RiboswitchTransformer, self).__init__()\n",
        "        \n",
        "        # Input Encoding Layers\n",
        "        self.embedding = nn.Embedding(num_embeddings=4, embedding_dim=128)\n",
        "        \n",
        "        # Graph Neural Network Layer\n",
        "        # Graph Construction\n",
        "        # Node and Edge Features\n",
        "        \n",
        "        # Graph Neural Network Architecture\n",
        "        self.gnn_layer = nn.ModuleList([\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        ])\n",
        "        \n",
        "        # Output Features\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        self.self_attn_layer = nn.MultiheadAttention(embed_dim=32, num_heads=4)\n",
        "        \n",
        "        # Positional Encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(100, 1, 32))\n",
        "        \n",
        "        # Feedforward Layer\n",
        "        self.feedforward_layer = nn.Sequential(\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Classification Layer\n",
        "        self.classification_layer = nn.Linear(32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Input Encoding Layers\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # Graph Neural Network Layer\n",
        "        # Graph Construction\n",
        "        # Node and Edge Features\n",
        "        \n",
        "        # Graph Neural Network Architecture\n",
        "        for layer in self.gnn_layer:\n",
        "            x = layer(x)\n",
        "        \n",
        "        # Output Features\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        x = x.transpose(0, 1)\n",
        "        x, _ = self.self_attn_layer(x, x, x)\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        # Positional Encoding\n",
        "        x = x + self.pos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMTWc7gqeK85"
      },
      "source": [
        "\n",
        "\n",
        "## Training (Transformer)\n",
        "\n",
        "We will train the model using PyTorch. This will involve defining a loss function, selecting an optimizer, and running multiple epochs of training. We will use a cross-entropy loss function and the Adam optimizer, which is a popular choice for deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKc8jBLj64fL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Use the RiboswitchDataset and DataLoader code you provided earlier\n",
        "\n",
        "# Define training and validation loop\n",
        "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.squeeze()).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "num_classes = len(riboswitch_df['rfam_id'].unique())\n",
        "model = TransformerModel(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "weight_decay = 1e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 50\n",
        "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFFSNWnPTWZJ"
      },
      "source": [
        "## Model Architecture (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLMIB5HlTl02"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, num_classes, input_size=4, hidden_size=128, num_layers=3, dropout=0.1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        x = self.dropout(hidden[-1])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Use the RiboswitchDataset and DataLoader code you provided earlier\n",
        "\n",
        "# Define training and validation loop\n",
        "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, label_names):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.squeeze())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels.squeeze()).sum().item()\n",
        "\n",
        "                # Store true labels and predicted labels\n",
        "                if epoch == num_epochs - 1:\n",
        "                    y_true.extend(labels.squeeze().tolist())\n",
        "                    y_pred.extend(predicted.tolist())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "    \n",
        "    return y_true, y_pred, label_names\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "num_classes = len(riboswitch_df['rfam_id'].unique())\n",
        "model = LSTMModel(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Add a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "# Train and evaluate the model\n",
        "num_epochs = 100\n",
        "label_names = riboswitch_df[\"rfam_id\"].unique()\n",
        "y_true, y_pred, label_names = train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, label_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4hbv3h3eMZR"
      },
      "source": [
        "## Evaluation (LSTM)\n",
        "\n",
        "We will evaluate the performance of the trained model using various metrics such as accuracy, precision, recall, and F1-score. We can also visualize the results using confusion matrices and ROC curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emMVokJFkIb-"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model_path = \"lstm_model_scripted.pt\"  # Choose the path and filename you want for your model\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9brU9J-7mCuu"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model_path = \"lstm_model_scripted.pt\"\n",
        "model = LSTMModel(num_classes=num_classes) # Replace this with the model class you used for training\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Move the model to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITK2MPneiUNB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Compute the accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=label_names, yticklabels=label_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "y_true_binary = label_binarize(y_true, classes=list(range(len(label_names))))\n",
        "y_pred_binary = label_binarize(y_pred, classes=list(range(len(label_names))))\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(len(label_names)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_binary[:, i], y_pred_binary[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "for i in range(len(label_names)):\n",
        "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'.format(label_names[i], roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49rzQMLmZzO4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the data\n",
        "data = {\n",
        "    'RF00050': 145,\n",
        "    'RF00059': 113,\n",
        "    'RF00080': 29,\n",
        "    'RF00162': 457,\n",
        "    'RF00167': 133,\n",
        "    'RF00168': 47,\n",
        "    'RF00174': 433,\n",
        "    'RF00234': 18,\n",
        "    'RF00379': 106,\n",
        "    'RF00380': 158,\n",
        "    'RF00442': 100,\n",
        "    'RF00504': 44,\n",
        "    'RF00521': 40,\n",
        "    'RF00522': 36,\n",
        "    'RF00634': 40,\n",
        "    'RF01054': 13,\n",
        "    'RF01055': 102,\n",
        "    'RF01056': 4,\n",
        "    'RF01057': 52,\n",
        "    'RF01482': 7,\n",
        "    'RF01510': 3,\n",
        "    'RF01689': 140,\n",
        "    'RF01704': 359,\n",
        "    'RF01725': 414,\n",
        "    'RF01727': 47,\n",
        "    'RF01734': 233,\n",
        "    'RF01739': 663,\n",
        "    'RF01750': 148,\n",
        "    'RF01767': 25,\n",
        "    'RF01786': 48,\n",
        "    'RF01826': 6,\n",
        "    'RF01831': 100,\n",
        "    'RF02680': 18,\n",
        "    'RF02683': 71,\n",
        "    'RF02885': 6,\n",
        "    'RF02912': 4,\n",
        "    'RF03057': 241,\n",
        "    'RF03058': 45,\n",
        "    'RF03071': 107,\n",
        "    'RF03072': 450\n",
        "}\n",
        "\n",
        "# Sort the data by frequency\n",
        "sorted_data = sorted(data.items(), key=lambda x: x[1])\n",
        "\n",
        "# Extract the x and y values\n",
        "x_values = [d[0] for d in sorted_data]\n",
        "y_values = [d[1] for d in sorted_data]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.bar(x_values, y_values)\n",
        "\n",
        "# Set the chart title and axis labels\n",
        "plt.title('Frequency of Rfam IDs')\n",
        "plt.xlabel('Rfam ID')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Rotate the x-axis labels for better visibility\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Show the chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlWV8WLcppUr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Binarize the output\n",
        "n_classes = len(np.unique(y_true))\n",
        "y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
        "y_pred_bin = label_binarize(y_pred, classes=range(n_classes))\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Print ROC curve data as text\n",
        "for i in range(n_classes):\n",
        "    print(f\"ROC curve of class {i}:\")\n",
        "    print(f\"FPR: {fpr[i]}\")\n",
        "    print(f\"TPR: {tpr[i]}\")\n",
        "    print(f\"AUC: {roc_auc[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-av37d6qbEu"
      },
      "source": [
        "# Evaluation (Transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iCT555m0R8f"
      },
      "source": [
        "# Other Code for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e49mH88p7MU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import gzip\n",
        "\n",
        "# Set search parameters\n",
        "url = 'https://rfam.org/search?q=entry_type:%22Family%22%20AND%20rna_type:%22riboswitch%22'\n",
        "num_per_page = 50\n",
        "\n",
        "# Initialize list to hold family details\n",
        "families = []\n",
        "\n",
        "# Loop over search results pages\n",
        "page_num = 0\n",
        "while True:\n",
        "    # Get search results page\n",
        "    search_url = f'{url}&start={page_num*num_per_page}&rows={num_per_page}'\n",
        "    response = requests.get(search_url)\n",
        "    search_results = json.loads(response.text)\n",
        "\n",
        "    # Break if no more pages\n",
        "    if not search_results['results']:\n",
        "        break\n",
        "    \n",
        "\n",
        "    # Loop over families on page\n",
        "    for result in search_results['results']:\n",
        "        # Check if family has RNA type riboswitch\n",
        "        if 'rna_type' in result and result['rna_type'] == 'riboswitch':\n",
        "            # Get family details\n",
        "            family = {\n",
        "                'rfam_id': result['rfam_acc'],\n",
        "                'name': result['name']\n",
        "            }\n",
        "\n",
        "            # Add family to list\n",
        "            families.append(family)\n",
        "    \n",
        "    # Loop over families on page\n",
        "    for result in search_results['results']:\n",
        "        # Check if family has RNA type riboswitch\n",
        "        if 'rna_type' in result and result['rna_type'] == 'riboswitch':\n",
        "            # Get family details\n",
        "            family_id = result['rfam_acc']\n",
        "            family_name = result['description']\n",
        "            family_url = f'https://rfam.org/family/{family_id}'\n",
        "            \n",
        "            # Download and extract the FASTA file\n",
        "            fasta_url = f'https://ftp.ebi.ac.uk/pub/databases/Rfam/CURRENT/fasta_files/{family_id}.fa.gz'\n",
        "            response = requests.get(fasta_url, stream=True)\n",
        "            with gzip.open(response.raw, 'rt') as f:\n",
        "                lines = f.readlines()\n",
        "            sequences = []\n",
        "            for i in range(0, len(lines), 2):\n",
        "                label = lines[i].strip()[1:]\n",
        "                sequence = lines[i+1].strip()\n",
        "                sequences.append((label, sequence))\n",
        "                \n",
        "            # Add family details to list\n",
        "            families.append((family_id, family_name, family_url, sequences))\n",
        "    \n",
        "    # Increment page number\n",
        "    page_num += 1\n",
        "\n",
        "# Write family details to CSV file\n",
        "with open('riboswitch_families.csv', 'w') as f:\n",
        "    f.write('RFAM ID,Family Name,URL,Label,Sequence\\n')\n",
        "    for family in families:\n",
        "        family_id = family[0]\n",
        "        family_name = family[1]\n",
        "        family_url = family[2]\n",
        "        sequences = family[3]\n",
        "        for sequence in sequences:\n",
        "            label = sequence[0]\n",
        "            seq = sequence[1]\n",
        "            f.write(f'{family_id},{family_name},{family_url},{label},{seq}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ESvJdoMnXKl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# Set search parameters\n",
        "#query = 'entry_type:\"Family\" AND rna_type:\"riboswitch\"'\n",
        "sort = 'default'\n",
        "num_per_page = 90\n",
        "\n",
        "# Initialize lists to hold sequence data and labels\n",
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "# Loop over search results pages\n",
        "page_num = 0\n",
        "while True:\n",
        "    # Get search results page\n",
        "    search_url = f'https://rfam.org/search?q=entry_type:%22Family%22%20AND%20rna_type:%22riboswitch%22&sort={sort}&start={page_num*num_per_page}&rows={num_per_page}'\n",
        "    try:\n",
        "        response = requests.get(search_url)\n",
        "        search_results = json.loads(response.text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while getting search results page: {e}\")\n",
        "        break\n",
        "    \n",
        "    print(response.text)\n",
        "    # Break if no more pages\n",
        "    if not search_results['results']:\n",
        "        break\n",
        "    \n",
        "    # Loop over families on page\n",
        "    for result in search_results['results']:\n",
        "        # Check if family has RNA type riboswitch\n",
        "        if 'rna_type' in result and result['rna_type'] == 'riboswitch':\n",
        "            # Get family description\n",
        "            family_url = f'http://rfam.org/family/{result[\"rfam_acc\"]}?content-type=application/json'\n",
        "            try:\n",
        "                response = requests.get(family_url)\n",
        "                family_data = response.json()\n",
        "\n",
        "                # Extract alignment URL from family description\n",
        "                alignment_url = family_data[\"rfam\"][\"alignment\"][\"url\"] + \"?content-type=text%2Fplain\"\n",
        "\n",
        "                # Retrieve alignment file\n",
        "                response = requests.get(alignment_url)\n",
        "                alignment_data = response.text\n",
        "\n",
        "                # Extract sequences from alignment file\n",
        "                for line in alignment_data.splitlines():\n",
        "                    if line.startswith(\">\"):\n",
        "                        label = line.strip(\">\").split()[0]\n",
        "                    else:\n",
        "                        sequence = torch.tensor([int(c == x) for x in \"ATCG\"] for c in line.strip())\n",
        "                        sequences.append(sequence)\n",
        "                        labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred while processing family {result['rfam_acc']}: {e}\")\n",
        "            \n",
        "    # Increment page number\n",
        "    page_num += 1\n",
        "\n",
        "# Pad sequences with zeros to make them the same length\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = []\n",
        "for seq in sequences:\n",
        "    padding = torch.zeros(max_len - len(seq), len(seq[0]))\n",
        "    padded_seq = torch.cat([seq, padding])\n",
        "    padded_sequences.append(padded_seq)\n",
        "\n",
        "# Create dataset\n",
        "dataset = torch.utils.data.TensorDataset(torch.stack(padded_sequences), labels)\n",
        "\n",
        "# Save dataset to file\n",
        "torch.save(dataset, 'riboswitch_dataset.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JnWdnBIUeMjY",
        "IUph_h90UcgU",
        "ZMTWc7gqeK85"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}